{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Test Loss: 0.3022 Micro F1 Score: 0.3724\n",
      "Epoch 2/100, Test Loss: 0.2874 Micro F1 Score: 0.4252\n",
      "Epoch 3/100, Test Loss: 0.2795 Micro F1 Score: 0.4393\n",
      "Epoch 4/100, Test Loss: 0.2746 Micro F1 Score: 0.4583\n",
      "Epoch 5/100, Test Loss: 0.2703 Micro F1 Score: 0.4956\n",
      "Epoch 6/100, Test Loss: 0.2664 Micro F1 Score: 0.4948\n",
      "Epoch 7/100, Test Loss: 0.2635 Micro F1 Score: 0.5007\n",
      "Epoch 8/100, Test Loss: 0.2611 Micro F1 Score: 0.5098\n",
      "Epoch 9/100, Test Loss: 0.2588 Micro F1 Score: 0.5037\n",
      "Epoch 10/100, Test Loss: 0.2566 Micro F1 Score: 0.5304\n",
      "Epoch 11/100, Test Loss: 0.2548 Micro F1 Score: 0.5365\n",
      "Epoch 12/100, Test Loss: 0.2530 Micro F1 Score: 0.5354\n",
      "Epoch 13/100, Test Loss: 0.2514 Micro F1 Score: 0.5436\n",
      "Epoch 14/100, Test Loss: 0.2500 Micro F1 Score: 0.5541\n",
      "Epoch 15/100, Test Loss: 0.2484 Micro F1 Score: 0.5623\n",
      "Epoch 16/100, Test Loss: 0.2475 Micro F1 Score: 0.5605\n",
      "Epoch 17/100, Test Loss: 0.2463 Micro F1 Score: 0.5539\n",
      "Epoch 18/100, Test Loss: 0.2449 Micro F1 Score: 0.5731\n",
      "Epoch 19/100, Test Loss: 0.2439 Micro F1 Score: 0.5755\n",
      "Epoch 20/100, Test Loss: 0.2425 Micro F1 Score: 0.5818\n",
      "Epoch 21/100, Test Loss: 0.2416 Micro F1 Score: 0.5774\n",
      "Epoch 22/100, Test Loss: 0.2408 Micro F1 Score: 0.5917\n",
      "Epoch 23/100, Test Loss: 0.2397 Micro F1 Score: 0.5819\n",
      "Epoch 24/100, Test Loss: 0.2388 Micro F1 Score: 0.5853\n",
      "Epoch 25/100, Test Loss: 0.2380 Micro F1 Score: 0.5911\n",
      "Epoch 26/100, Test Loss: 0.2371 Micro F1 Score: 0.5977\n",
      "Epoch 27/100, Test Loss: 0.2363 Micro F1 Score: 0.6062\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from CSV\n",
    "csv_path = '../data/raw/R6_train.csv'  # Update with your CSV file path\n",
    "data = pd.read_csv(csv_path)\n",
    "labels = pd.read_csv('../data/raw/labels_train.csv')\n",
    "\n",
    "# Assuming your CSV has a column 'target' for the 19 binary labels and other columns as features\n",
    "X = data.values\n",
    "y = labels.values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Set the input, hidden, and output sizes\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = input_size * 10\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CustomModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "            all_predictions.append(predictions.numpy())\n",
    "            all_targets.append(labels.numpy())\n",
    "\n",
    "        average_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "        # Calculate micro F1 score\n",
    "        all_predictions = np.concatenate(all_predictions)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        micro_f1 = f1_score(all_targets, (all_predictions > 0.5).astype(int), average='micro')\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Test Loss: {average_test_loss:.4f} Micro F1 Score: {micro_f1:.4f}')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
